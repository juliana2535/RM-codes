{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The MIT License (MIT)\n",
    "\n",
    "#Copyright (c) 2020 Juliana T.C. Marcos\n",
    "\n",
    "#THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO\n",
    "#THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n",
    "#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF \n",
    "#CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "#This code use methods of the ParticleFilter (PF) class in order to track an object in a video. The PF uses two\n",
    "#measurements providers which are intermitently chosen based on the value of the ssim index of two images. The\n",
    "#first image is a template of the animal to track while the second is the Region of Interest (ROI) surrounding\n",
    "#the previous estimated location of the tracked animal. The measurements providers are a colour image segmentation\n",
    "#technique and the You Only Look Once (YOLO) object detector. \n",
    "\n",
    "#The particles are represented in red, the PF estimate of\n",
    "#the tracked animal is represented in green and the animal's bb is represented in blue. \n",
    "\n",
    "#Thanks to Nayak for the nice tutorial about using YOLOv3 with OpenCv which is available at this address:\n",
    "#https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import of useful librairies\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from skimage import measure\n",
    "from ParticleFilter import ParticleFilter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some variables initialization \"\"\"\n",
    "\n",
    "#The total number of trials\n",
    "Tot=1\n",
    "#Lists for the Tot running outputs averages \n",
    "BB_avg=[]\n",
    "ROI_avg=[]\n",
    "anchor_avg=[]\n",
    "xy_est_avg=[]\n",
    "ssim_avg=[]\n",
    "particles_avg=[]\n",
    "#number of particles\n",
    "n_particles=2000\n",
    "#noise in sensors' measurements\n",
    "meas_noise=0\n",
    "#Read weights and config files to create YOLO(v3) net\n",
    "net = cv2.dnn.readNet(\"./Inputs/yolov3.weights\", \"./Inputs/yolov3.cfg\")\n",
    "#Fetch the three output layers names of YOLO net, they are the ones not connected to \n",
    "#any following layers since they are the last layers\n",
    "layer_names = net.getLayerNames()\n",
    "output_layer_names = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "#Read the classes in coco.names file for YOLO net\n",
    "coco_classes = []\n",
    "with open ('./Inputs/coco.names','r') as file:\n",
    "    coco_classes=[line.strip(\"\\n\") for line in file.readlines()]\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "text_coord=(10,40)\n",
    "t_size=0.8\n",
    "t_thick=2\n",
    "#This value was chosen according to a paper experiment\n",
    "N_thresh=(2*n_particles)/30\n",
    "#Scale for YOLO's inputs preprocessing\n",
    "scale=1/255\n",
    "#Video frame width and height\n",
    "frame_width=1920\n",
    "frame_height=1080\n",
    "#Lists to contain the counters for each trial\n",
    "cis_l=[]\n",
    "yolo_l=[]\n",
    "yolo_det_l=[]\n",
    "n_resampl_l=[]\n",
    "#Initialize variable for shifting the anchor update between first measurements and first estimations\n",
    "anchor_shift=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data for video cows\n",
    "anchorS=(1250,350)\n",
    "#This threshold helps to filter small contours\n",
    "#It is however important to adapt it to the object scales in the videos\n",
    "Area_thresh=0\n",
    "#This is to choose the type of threshold (between cv2.THRESH_BINARY_INV \n",
    "#and THRESH_BINARY)\n",
    "type_thr=cv2.THRESH_BINARY_INV\n",
    "videoIn_name=\"./Inputs/cows.avi\"\n",
    "path=\"./Outputs/\"\n",
    "videoOut_name=path+\"cows-pf-ssim-colour-yolo.avi\"\n",
    "template = cv2.imread('./Inputs/template2.png')\n",
    "template2 = cv2.imread('./Inputs/template2_grass.png')\n",
    "#std in the prediction of particles for the object's position\n",
    "std=10\n",
    "chg_thres=0.60\n",
    "#These are YOLO parameters\n",
    "conf=0.8\n",
    "nms=0.7\n",
    "height, width, channels =224,320,3\n",
    "blob_x,blob_y=224,320\n",
    "handover=path+\"handpics/cows\"\n",
    "#Motion model's speed in x and y directions\n",
    "v_x=0.01\n",
    "v_y=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fyi,fxi=template2.shape[0],template2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56801,
     "status": "ok",
     "timestamp": 1579782042429,
     "user": {
      "displayName": "Juliana Thomasia Chakirath Marcos",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBi57pn5YLYNbvs4CWoUwr5qczRkiGsQZoz-qFs=s64",
      "userId": "11545008696825308510"
     },
     "user_tz": -120
    },
    "id": "uph0UIjRbSHH",
    "outputId": "19e9b6ef-284a-4a79-b6b7-1e51d3ec0078",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for num in range(Tot):\n",
    "    #List for averaging running outputs\n",
    "    BB_l=[]\n",
    "    ROI_l=[]\n",
    "    anchor_l=[]\n",
    "    xy_est_l=[]\n",
    "    ssim_l=[]\n",
    "    particles_l=[]\n",
    "    #Counters for call of cis, yolo and yolo actual detection\n",
    "    cis=0\n",
    "    yolo=0\n",
    "    yolo_det=0\n",
    "    it=0\n",
    "    n_resampling=0\n",
    "    #A single program variables initialization\n",
    "    anchor=anchorS\n",
    "    #Initialization of measurements variables\n",
    "    x_objMeasure,y_objMeasure=anchor[0],anchor[1]\n",
    "    #Initialization of some important variables\n",
    "    ssim=0\n",
    "    #BB variable initialization\n",
    "    BB=0,0,0,0\n",
    "    #Capture video where object tracking should be performed\n",
    "    video = cv2.VideoCapture(videoIn_name)\n",
    "    #Video output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    video_output = cv2.VideoWriter(videoOut_name,fourcc,60,(frame_width,frame_height))\n",
    "    #Particles Instantiation\n",
    "    particles=ParticleFilter(frame_width,frame_height, n_particles)\n",
    "\n",
    "    #This function uses opencv functions to identify the center of the animal to track\n",
    "    def sensors_measurements(template,img,anchor,meas_noise=3,kernel=3):\n",
    "\n",
    "        #Compute the mean color of the template containing the animal to track color\n",
    "        meanStdTemplate=cv2.meanStdDev(template)\n",
    "\n",
    "        #Convert the color of the frame to work on and apply the GaussianBlur function in order to remove\n",
    "        #Gaussian noise, smooth image and somrtimes highlight edges\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.GaussianBlur(img, (kernel, kernel), 0)\n",
    "\n",
    "        #Binarize frame using the color mean of the animal in such a way that matching parts\n",
    "        #of the frame will appear white and other parts will appear black\n",
    "        #Use the green channel to find the threshold value since it works well empirically\n",
    "        ret,thresh = cv2.threshold(img,meanStdTemplate[0][1],255,type_thr)\n",
    "\n",
    "        # find contours in the thresholded image. The if condition is used to avoid error that happens depending\n",
    "        #on the python version used. The middle parameter is to only retrieve parent contours while the latter\n",
    "        #is to avoid redundant points in contours.\n",
    "        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "        #Compute the contours areas\n",
    "        contoursAreas=[cv2.contourArea(c) for c in contours]\n",
    "\n",
    "        #Compute the center and the BB of the contours\n",
    "        contoursCenter=[]\n",
    "        contoursBB=[]\n",
    "\n",
    "        for c in range(len(contours)):\n",
    "            M = cv2.moments(contours[c])\n",
    "            #If the area of the current contour exists and is greater than a threshold for areas \n",
    "            if M[\"m00\"] != 0 and contoursAreas[c]>Area_thresh:\n",
    "                cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "                contoursCenter.append((c,cX,cY,contoursAreas[c]))\n",
    "                contoursBB.append(cv2.boundingRect(contours[c]))\n",
    "\n",
    "        #Compute the distance between each center and the anchor center in order to find \n",
    "        #the most suitable shape to track i.e the closest one to the anchor\n",
    "\n",
    "        contoursAnchorDist=[math.sqrt((contoursCenter[i][1]-anchor[0])**2+\\\n",
    "        (contoursCenter[i][2]-anchor[1])**2) for i in range (len(contoursCenter))]\n",
    "\n",
    "        #Save and return the coordinates of the most suitable center and its contours\n",
    "        index=contoursAnchorDist.index(min(contoursAnchorDist))\n",
    "        cX=contoursCenter[index][1]+np.random.standard_normal()*meas_noise\n",
    "        cY=contoursCenter[index][2]+np.random.standard_normal()*meas_noise\n",
    "\n",
    "        return cX,cY,contoursBB[index]\n",
    "    \n",
    "    #Loop through the entire video\n",
    "    while (True):\n",
    "        #Take the video and break it frame by frame\n",
    "        _,frame=video.read()\n",
    "        #Check if frames are captured\n",
    "        if(_ == False ): break\n",
    "\n",
    "        it+=1\n",
    "        #Particles prediction update\n",
    "        particles.particles_update(v_x,v_y,std,frame_width,frame_height)\n",
    "        particles_l.append(particles.particles.copy())\n",
    "        anchor_l.append(anchor)\n",
    "    \n",
    "        #Take the ROI around the animal in the current frame for the SSIM evaluation \n",
    "        #with the template and its dimensions\n",
    "        ROI_ssim=(int(anchor[0]-fxi/2),int(anchor[1]-fyi/2),fxi,fyi)\n",
    "        frame_crop=frame[ROI_ssim[1]:ROI_ssim[1]+ROI_ssim[3],ROI_ssim[0]:ROI_ssim[0]+ROI_ssim[2]]\n",
    "        fy,fx=frame_crop.shape[0],frame_crop.shape[1]\n",
    "    \n",
    "        #Here, we check if the dimensions of the ROI around the animal and the template are the same\n",
    "        #to select the cis if not, because these cases correspond to the frame boundaries. Therefore, \n",
    "        #the ROI might not contain sufficient information to allow the change detection with the ssim.\n",
    "        if ((fxi!=fx) or (fyi!=fy)):\n",
    "            ssim=1.1\n",
    "        else: \n",
    "            #The value 3 is the minimum dimension for images to compute the ssim index\n",
    "            if ((fxi>3 and fyi>3)): \n",
    "                ssim=measure.compare_ssim(frame_crop,template2, \\\n",
    "                                              multichannel=True,win_size=3)\n",
    "            else:\n",
    "                ssim=0.0\n",
    "        \n",
    "        ssim=abs(ssim)\n",
    "        ssim_l.append(ssim)\n",
    "        \n",
    "        if (ssim<chg_thres):\n",
    "            \n",
    "            yolo+=1\n",
    "\n",
    "            #Coordinates of ROI around anchor\n",
    "            dy1=int(anchor[1]-height/2)\n",
    "            #if(dy1<0):dy1=0\n",
    "            dy2=int(anchor[1]+height/2)\n",
    "            #if (dy2>frame_height):dy2=frame_height\n",
    "            dx1=int(anchor[0]-width/2)\n",
    "            #if (dx1<0): dx1=0\n",
    "            dx2=int(anchor[0]+width/2)\n",
    "            #if (dx2>frame_width): dx2=frame_width\n",
    "\n",
    "            frame_roi=frame[dy1:dy2,dx1:dx2]\n",
    "            ROI_l.append((dx1,dy1,dx2,dy2))\n",
    "            \n",
    "            # Detecting objects\n",
    "            blob = cv2.dnn.blobFromImage(frame_roi,scale,(blob_x, blob_y),(0, 0, 0), True, crop=False)\n",
    "            net.setInput(blob)\n",
    "            #Run forward pass to get predictions from output layers selected\n",
    "            outputs = net.forward(output_layer_names)\n",
    "            # Showing informations on the screen\n",
    "            classes = []\n",
    "            confidences = []\n",
    "            boxes = []\n",
    "            yolo_center=[]\n",
    "            yolo_center_anchor_dist=[]\n",
    "\n",
    "            #for each output of the YOLO last layers\n",
    "            for output in outputs:\n",
    "                for detection in output:\n",
    "                #Each detection contains center point x,y,width,height,object probability\n",
    "                #and 80 class probability\n",
    "                    scores = detection[5:]\n",
    "                    class_id = np.argmax(scores)\n",
    "                    confidence = scores[class_id]\n",
    "                    if confidence > conf:\n",
    "                        print(\"step \",str(it),\"confidence \",confidence,\"\\n\")\n",
    "                        #non normalized detected object's center, width and height \n",
    "                        center_x,center_y = int(detection[0] *blob_x),int(detection[1] * blob_y)\n",
    "                        w,h = int(detection[2] * blob_x),int(detection[3] * blob_y)\n",
    "                        # Rectangle coordinates\n",
    "                        x,y = int(center_x - w / 2),int(center_y - h / 2)\n",
    "                        boxes.append([x, y, w, h])\n",
    "                        confidences.append(float(confidence))\n",
    "                        classes.append(class_id)\n",
    "\n",
    "                        #Get coordinates in initial video frame reference instead of \n",
    "                        #ROI reference\n",
    "                        center_x_reverse=center_x+(anchor[0]-width/2)\n",
    "                        center_y_reverse=center_y+(anchor[1]-height/2)\n",
    "                        yolo_center.append((center_x_reverse,center_y_reverse))\n",
    "\n",
    "            #Search and keep relevant bounding boxes given their scores \n",
    "            indexes = cv2.dnn.NMSBoxes(boxes, confidences, conf, nms)\n",
    "            #Only compute the distances between anchor and the yolo predictions\n",
    "            #if confidence is good and indexes is not empty\n",
    "            if (len(indexes)!=0):\n",
    "                yolo_center_anchor_dist=[math.sqrt((yolo_center[i][0]-anchor[0])**2+\\\n",
    "                (yolo_center[i][1]-anchor[1])**2) for i in range (len(yolo_center))\\\n",
    "                                        if i in indexes]\n",
    "                #Keep the relevant indices in a list to retrieve the relevant index latter\n",
    "                nms_indexes=[i for i in range (len(yolo_center)) if i in indexes]\n",
    "\n",
    "                print(\"step \",str(it),\"indexes \",indexes,\" nms \",nms_indexes,\\\n",
    "                \" yolo_center \",yolo_center,\" distances \", yolo_center_anchor_dist, \"\\n\")\n",
    "\n",
    "                #Save and return the coordinates of the most suitable center and its BB\n",
    "                #Keep the index of the minimum distance between prediction and anchor\n",
    "                index=yolo_center_anchor_dist.index(min(yolo_center_anchor_dist))\n",
    "                #Retrieve relevant index to select the yolo index and the corresponding BB\n",
    "                index_yolo=nms_indexes[index]\n",
    "                print(\"step \",str(it),\" index \",index,\" index_yolo \",index_yolo,\"\\n\")\n",
    "                BB_x=int(boxes[index_yolo][0]+(anchor[0]-width/2))\n",
    "                BB_y=int(boxes[index_yolo][1]+(anchor[1]-height/2))\n",
    "                BB =BB_x,BB_y,boxes[index_yolo][2],boxes[index_yolo][3]\n",
    "\n",
    "                #Update measurements  \n",
    "                x_objMeasure,y_objMeasure=yolo_center[index_yolo][0],yolo_center[index_yolo][1]\n",
    "                \n",
    "                yolo_det+=1\n",
    "            \n",
    "            cv2.rectangle(frame, (dx1,dy1), (dx2,dy2), (0,0,255), 2)\n",
    "            cv2.putText(frame,'Change detected call yolo: SSIM = '+str(ssim),text_coord,font,t_size,(255,255,255),t_thick,cv2.LINE_AA)   \n",
    "\n",
    "        else :\n",
    "            #Measurements\n",
    "            x_objMeasure,y_objMeasure,BB = sensors_measurements(template,frame,anchor,meas_noise)\n",
    "            #Use anchor to select the closest area and then the actual tracked target\n",
    "            cis+=1\n",
    "            ROI_l.append((0,0,0,0))\n",
    "            cv2.putText(frame,'No Change detected, use CIS: SSIM = '+str(ssim),text_coord,font,t_size,(255,255,255),t_thick,cv2.LINE_AA)   \n",
    "\n",
    "        #Save BB in a list\n",
    "        BB_l.append(BB)\n",
    "        \n",
    "        #Update the particles weights with the new measurement\n",
    "        particles.weigth_update(x_objMeasure,y_objMeasure)\n",
    "\n",
    "        #Estimation of object center position\n",
    "        x_estimation,y_estimation=particles.position_estimation()\n",
    "\n",
    "        if it%200==0:\n",
    "            np.save(handover+\"_estimation_data2_\"+str(it)+\".npy\",np.array([x_estimation,y_estimation]))\n",
    "            np.save(handover+\"_particles_data2_\"+str(it)+\".npy\",np.array(particles.particles))\n",
    "            np.save(handover+'_weights_data2_'+str(it)+'.npy',np.array(particles.weights))\n",
    "            cv2.imwrite(handover+'_HandoverA_data2_'+str(it)+'.png',frame)\n",
    "        \n",
    "\n",
    "        #Save the x and y estimated in a list\n",
    "        xy_est_l.append((x_estimation,y_estimation))\n",
    "\n",
    "        #Draw the particles\n",
    "        particles.draw_box_particles(frame,BB,x_estimation,y_estimation)\n",
    "        #Draw the position estimation\n",
    "        cv2.circle(frame,(x_estimation,y_estimation),5,[0,255,0],3)\n",
    "       \n",
    "        #Update the anchor with either the current measurements or the x and y estimates\n",
    "        if it > anchor_shift:\n",
    "            anchor=x_estimation,y_estimation\n",
    "        else:\n",
    "            anchor=x_objMeasure,y_objMeasure\n",
    "        \n",
    "        #Resample the particles\n",
    "        if (particles.effective_particles() < N_thresh):\n",
    "            n_resampling+=1\n",
    "            particles.resampling()\n",
    "            \n",
    "        #Reinitialize BB to avoid cis bb drawing when yolo does not detect\n",
    "        BB=0,0,0,0\n",
    "\n",
    "        if (num==Tot-1):\n",
    "            video_output.write(frame)\n",
    "\n",
    "        #if it==100:\n",
    "        #    break\n",
    "    \n",
    "    cis_l.append(cis)\n",
    "    yolo_l.append(yolo)\n",
    "    yolo_det_l.append(yolo_det) \n",
    "    n_resampl_l.append(n_resampling)\n",
    "    BB_avg.append(BB_l)\n",
    "    xy_est_avg.append(xy_est_l)\n",
    "    ssim_avg.append(ssim_l)\n",
    "    particles_avg.append(particles_l)\n",
    "    ROI_avg.append(ROI_l)\n",
    "    anchor_avg.append(anchor_l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1290,
     "status": "ok",
     "timestamp": 1579782048265,
     "user": {
      "displayName": "Juliana Thomasia Chakirath Marcos",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBi57pn5YLYNbvs4CWoUwr5qczRkiGsQZoz-qFs=s64",
      "userId": "11545008696825308510"
     },
     "user_tz": -120
    },
    "id": "eIdZ-33dYmtS",
    "outputId": "b8a08eda-c1e4-46be-a506-ef7c061f2860"
   },
   "outputs": [],
   "source": [
    "prog_duration= time.time() - start_time\n",
    "prog_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1413,
     "status": "ok",
     "timestamp": 1579782050004,
     "user": {
      "displayName": "Juliana Thomasia Chakirath Marcos",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBi57pn5YLYNbvs4CWoUwr5qczRkiGsQZoz-qFs=s64",
      "userId": "11545008696825308510"
     },
     "user_tz": -120
    },
    "id": "h1exhSim-F-k",
    "outputId": "d50940e0-7762-4329-9ad2-57a25f88ba17"
   },
   "outputs": [],
   "source": [
    "prog_duration/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_duration/(60*Tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(n_resampl_l)/Tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BB_l),len(xy_est_l),len(BB_avg),len(xy_est_avg),len(particles_avg),len(ssim_avg),\\\n",
    "len(anchor_avg),len(ROI_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cis_l)/Tot,sum(yolo_l)/Tot,sum(yolo_det_l)/Tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_l_avg=[] \n",
    "BB_avg=np.array(BB_avg)\n",
    "BB_l_avg=np.sum(BB_avg,0)/Tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_est_l_avg=[] \n",
    "xy_est_avg=np.array(xy_est_avg)\n",
    "xy_est_l_avg=np.sum(xy_est_avg,0)/Tot\n",
    "xy_est_l_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_l_avg=[] \n",
    "ssim_avg=np.array(ssim_avg)\n",
    "ssim_l_avg=np.sum(ssim_avg,0)/Tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI_l_avg=[] \n",
    "ROI_avg=np.array(ROI_avg)\n",
    "ROI_l_avg=np.sum(ROI_avg,0)/Tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles_l_avg=[] \n",
    "particles_avg=np.array(particles_avg)\n",
    "particles_l_avg=np.sum(particles_avg,0)/Tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_l_avg=[] \n",
    "anchor_avg=np.array(anchor_avg)\n",
    "anchor_l_avg=np.sum(anchor_avg,0)/Tot\n",
    "anchor_l_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xy_est_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BB_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(path+'xy_data2.npy',np.array(xy_est_l_avg))\n",
    "np.save(path+'BB_data2.npy',np.array(BB_l_avg))\n",
    "np.save(path+'ssim_data2.npy',np.array(ssim_l_avg))\n",
    "np.save(path+'ROI_data2.npy',np.array(ROI_l_avg))\n",
    "np.save(path+'part_data2.npy',np.array(particles_l_avg))\n",
    "np.save(path+'anchor_data2.npy',np.array(anchor_l_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ParticleFilter-data-yolo-roi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
